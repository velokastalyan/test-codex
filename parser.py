#!/usr/bin/env python
# -*- coding: utfâ€‘8 -*-

"""
ĞŸĞ°Ñ€ÑĞµÑ€ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³Ğ° https://sprintâ€‘rowery.pl/rowery
Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚: category, title, price, link
Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ² sprint_rowery.csv + sprint_rowery.xlsx

Ğ—Ğ°Ğ¿ÑƒÑĞº:
    python parser.py

ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚: Ctrl+C â€” ÑĞºÑ€Ğ¸Ğ¿Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞ¶Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ¾.
"""

import locale
import sys
locale.setlocale(locale.LC_ALL, '')                 # ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ ÑĞ½Ğ¸ĞºĞ¾Ğ´Ğ° Ğ² macOS / Linux
sys.stdout.reconfigure(encoding='utf-8')

import time
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin

import pandas as pd
import requests
from bs4 import BeautifulSoup

# â”€â”€â”€ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_URL   = "https://sprint-rowery.pl/rowery?product_list_limit=60"
HEADERS    = {"User-Agent": "Mozilla/5.0"}
MAX_WORKERS = 64                    # Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸
TIMEOUT     = 20
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def get_soup(url: str) -> BeautifulSoup:
    """Ğ—Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµĞ¼ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚ BeautifulSoup."""
    r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
    r.raise_for_status()
    return BeautifulSoup(r.text, "html.parser")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¿Ğ»Ğ¸Ñ‚ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_tile(tile, page_url: str) -> dict:
    title_el = tile.select_one("h3.product-item__name_heading a.product-item-link")
    price_el = tile.select_one("div.product-price-final-price span.price")
    link_el  = tile.select_one("a.product-item-link")

    return {
        "title": title_el.get_text(" ", strip=True) if title_el else "",
        "price": price_el.get_text(" ", strip=True) if price_el else "",
        "link" : urljoin(page_url, link_el["href"]) if link_el and link_el.has_attr("href") else "",
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_category(url: str) -> str:
    if not url:
        return ""

    try:
        soup = get_soup(url)
    except Exception:
        return ""

    for script in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(script.string or "")
        except Exception:
            continue

        # Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑÑƒĞµÑ‚ BreadcrumbList
        if isinstance(data, dict) and data.get("@type") == "BreadcrumbList":
            names = [
                el.get("item", {}).get("name", "")
                for el in data.get("itemListElement", [])
                if isinstance(el, dict)
            ]
            category = " > ".join(n for n in names if n)
            return category or ""

    return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ¿Ğ°Ñ€Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_page(url: str) -> tuple[list[dict], str | None]:
    print(f"â†·  {url}")
    soup = get_soup(url)

    tiles = soup.select("div.product-item-info")
    print(f"   ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞµĞº Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ: {len(tiles)}")

    items = [parse_tile(t, url) for t in tiles]

    # Ğ¿Ğ¾Ğ´Ñ‚ÑĞ³Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:
        fut2idx = {pool.submit(fetch_category, it["link"]): i
                   for i, it in enumerate(items) if it["link"]}

        for fut in as_completed(fut2idx):
            items[fut2idx[fut]]["category"] = fut.result()

    nxt = soup.select_one("li.pages-item-next > a, a.action.next")
    next_url = urljoin(url, nxt["href"]) if nxt and nxt.has_attr("href") else None
    return items, next_url


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ñ…Ğ¾Ğ´ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawl(start_url: str) -> list[dict]:
    all_items, url = [], start_url
    while url:
        page_items, url = parse_page(url)
        all_items.extend(page_items)
    return all_items


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save(data: list[dict]):
    df = pd.DataFrame(data)
    df.to_csv("sprint_rowery.csv",  sep=";", index=False, encoding="utf-8-sig")
    df.to_excel("sprint_rowery.xlsx", index=False)   # Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ openpyxl
    print(f"ğŸ“—  Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾  {len(data)} Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ².")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ²Ñ…Ğ¾Ğ´Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    t0 = time.time()
    collected: list[dict] = []

    try:
        collected = crawl(BASE_URL)
    except KeyboardInterrupt:
        print("\nâ¹  ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ â€“ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¿ĞµĞ» ÑĞ¾Ğ±Ñ€Ğ°Ñ‚ÑŒâ€¦")
    finally:
        if collected:
            save(collected)
        else:
            print("âš ï¸  ĞĞµÑ‡ĞµĞ³Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ â€“ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¿ÑƒÑÑ‚.")

    print(f"â±  Ğ’Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹: {time.time() - t0:.1f}Â ÑĞµĞº.")
