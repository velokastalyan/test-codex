#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
–ü–∞—Ä—Å–µ—Ä –∫–∞—Ç–∞–ª–æ–≥–∞ https://sprint-rowery.pl/rowery
–°–æ–±–∏—Ä–∞–µ—Ç: category, title, price, link
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ sprint_rowery.csv  +  sprint_rowery.xlsx

–ó–∞–ø—É—Å–∫:
    python parser.py
–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤ –ª—é–±–æ–π –º–æ–º–µ–Ω—Ç: Ctrl+C  ‚Äì  —Å–∫—Ä–∏–ø—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç —Ç–æ, —á—Ç–æ —É–∂–µ —Å–æ–±—Ä–∞–Ω–æ.
"""

import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin

import pandas as pd
import requests
from bs4 import BeautifulSoup

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
BASE_URL    = "https://sprint-rowery.pl/rowery?product_list_limit=60"
HEADERS     = {"User-Agent": "Mozilla/5.0"}
MAX_WORKERS = 64            # –ø–æ—Ç–æ–∫–æ–≤ –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–∞
TIMEOUT     = 20
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def get_soup(url: str) -> BeautifulSoup:
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—ä–µ–∫—Ç BeautifulSoup."""
    r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
    r.raise_for_status()
    return BeautifulSoup(r.text, "html.parser")


# ---------- –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–ª–∏—Ç–∫–∏ ----------
def parse_tile(tile, page_url: str) -> dict:
    title_el = tile.select_one("h3.product-item__name__heading, a.product-item-link")
    price_el = tile.select_one("div.product-price-final-price, span.price")
    link_el  = tile.select_one("a.product-item-link")

    return {
        "title": title_el.get_text(" ", strip=True) if title_el else "",
        "price": price_el.get_text(" ", strip=True) if price_el else "",
        "link":  urljoin(page_url, link_el["href"]) if link_el and link_el.has_attr("href") else "",
    }


# ---------- –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Ç–æ–≤–∞—Ä–∞ ----------
def fetch_category(url: str) -> str:
    if not url:
        return ""
    try:
        soup = get_soup(url)
    except Exception:
        return ""
    crumbs = soup.select("ol.breadcrumbs li a")
    # –ø—Ä–æ–ø—É—Å—Ç–∏–º ¬´Start¬ª –∏ —Å–∞–º —Ç–æ–≤–∞—Ä (–ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç)
    return " > ".join(c.get_text(strip=True) for c in crumbs[1:-1]) if crumbs else ""


# ---------- –ø–∞—Ä—Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞ ----------
def parse_page(url: str) -> tuple[list[dict], str | None]:
    print(f"‚Üí {url}")
    soup = get_soup(url)

    # –ø–ª–∏—Ç–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤
    items = [parse_tile(t, url) for t in soup.select("div.product-item-info")]

    # –ø–æ–¥—Ç—è–≥–∏–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:
        fut2idx = {pool.submit(fetch_category, it["link"]): i
                   for i, it in enumerate(items) if it["link"]}
        for fut in as_completed(fut2idx):
            items[fut2idx[fut]]["category"] = fut.result()

    # —Å—Å—ã–ª–∫–∞ ¬´–î–∞–ª—å—à–µ¬ª
    nxt = soup.select_one("li.pages-item-next > a, a.action.next")
    next_url = urljoin(url, nxt["href"]) if nxt and nxt.has_attr("href") else None
    return items, next_url


# ---------- –ø–æ–ª–Ω—ã–π –æ–±—Ö–æ–¥ ----------
def crawl(start_url: str) -> list[dict]:
    all_items, url = [], start_url
    while url:
        page_items, url = parse_page(url)
        all_items.extend(page_items)
    return all_items


# ---------- save ----------
def save(data: list[dict]):
    df = pd.DataFrame(data)
    df.to_csv("sprint_rowery.csv", sep=";", index=False, encoding="utf-8-sig")
    df.to_excel("sprint_rowery.xlsx", index=False)


# ---------- –∑–∞–ø—É—Å–∫–∞–µ–º ----------
if __name__ == "__main__":
    t0 = time.time()
    collected: list[dict] = []

    try:
        collected = crawl(BASE_URL)            # –æ—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ö–æ–¥
    except KeyboardInterrupt:
        print("\n‚èπ  –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤—Ä—É—á–Ω—É—é ‚Äî —Å–æ—Ö—Ä–∞–Ω—è—é —Ç–æ, —á—Ç–æ —É—Å–ø–µ–ª —Å–æ–±—Ä–∞—Ç—å‚Ä¶")
    finally:
        if collected:
            save(collected)
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(collected)} —Ç–æ–≤–∞—Ä–æ–≤.")
        else:
            print("‚ö†Ô∏è  –ù–µ—á–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å ‚Äî —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç.")

        print(f"‚è±  –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {time.time() - t0:.1f}¬†—Å–µ–∫.")
